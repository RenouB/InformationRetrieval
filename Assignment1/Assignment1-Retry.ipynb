{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    A Node class for use in a linked list.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, value=None):\n",
    "        \n",
    "        self.value = value\n",
    "        self.next = None\n",
    "\n",
    "class LinkedList:\n",
    "    \"\"\"\n",
    "    A class to efficiently link our postings together.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, value):\n",
    "        \n",
    "        self.head = Node(value)\n",
    "\n",
    "\n",
    "    def at_end(self, new_value):\n",
    "        \"\"\"\n",
    "        Adds a new node at end of list.\n",
    "        \"\"\"        \n",
    "        new_node = Node(new_value)\n",
    "        \n",
    "        if self.head is None:\n",
    "            self.head = new_node\n",
    "            \n",
    "            return\n",
    "        \n",
    "        node = self.head\n",
    "        \n",
    "        while(node.next):\n",
    "            node = node.next\n",
    "            \n",
    "        node.next = new_node\n",
    "\n",
    "# Print the linked list\n",
    "\n",
    "    def to_list(self):\n",
    "        \"\"\"\n",
    "        Get the LinkedList as a normal list.\n",
    "        :return as_list: LinkedList as normal list.\n",
    "        \"\"\"\n",
    "        \n",
    "        listval = self.head\n",
    "        as_list = []\n",
    "        \n",
    "        while listval is not None:\n",
    "            as_list.append(listval.value)\n",
    "            listval = listval.next\n",
    "        \n",
    "        return as_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    \n",
    "    def __init__ (self, file):\n",
    "        \n",
    "        self.index, self.postings_list, self.tweets_by_id = self.create_index(file)\n",
    "        \n",
    "    def clean(self, token):\n",
    "        \"\"\"\n",
    "        Cleans each token before adding it to index.\n",
    "        :param token: the token to be cleaned.\n",
    "        :return: token\n",
    "        \"\"\"\n",
    "        \n",
    "        # Here we expand sequences of contractions. This will later help with language detection.\n",
    "        \n",
    "        contractions = [\"it's\", \"he's\",\"she's\",\"that's\", \"what's\", \"there's\",\\\n",
    "                        \"[newline]\", \"'m\", \"'ve\",\"n't\", \"'ll\",\"'re\", \"won't\", \"'d\", \"'s\"]\n",
    "        \n",
    "        fixes = [\"it is\", \"he is\",\"she is\",\"that is\", \"what is\", \"there is\",\\\n",
    "                 \" \", \" am\", \" have\", \" not\", \" will\", \" are\", \"will not\", \" would\", \"\"]\n",
    "        \n",
    "        for i in range(len(contractions)):\n",
    "            if contractions[i] in token:\n",
    "                token = token.replace(contractions[i], fixes[i])\n",
    "                \n",
    "        # Here we apply a series of regexes to get rid of URLS, emoticons, digits, etc. It could be cool to \n",
    "        # write a comment for each one, describing what it seeks to remove. \n",
    "        \n",
    "        token = re.sub(r'[^\\w\\s]', ' ' , token)\n",
    "        token = re.sub(r'[0-9].*\\s', ' ' , token)\n",
    "        token = re.sub(r'https?.+\\s', ' ' , token)\n",
    "        token = re.sub(r'[\\W].+[^\\W\\s]+|[^ ]+\\.[^ ]+ |[^a-zA-Zäöüß\\s]+ \\\n",
    "                         | \\d+|[^\\w\\s]+.[^\\W\\s]+| https?','', token)\n",
    "        \n",
    "        # Here we perform a check to make sure that at least one non space character has survived the\n",
    "        # cleaning process.\n",
    "        \n",
    "        if len(token) == 0 or token.isspace():\n",
    "            raise Exception('empty token') \n",
    "        \n",
    "        # Finally, we return the lowercase'd token.\n",
    "        \n",
    "        return token.lower()\n",
    "        \n",
    "    def create_index(self, file):\n",
    "        \"\"\"\n",
    "        Method to create index, postings list and list of tweets with associated IDs.\n",
    "        :param: file. path to tweets.csv file.\n",
    "        :return index: a dictionary of sorted terms, of which the values a list containing\n",
    "        frequency and a pointer to their postings list.\n",
    "        :return postings_list: contains all docids accompanying each term in our index.\n",
    "        :return tweets_by_id: a dictionary of tweet ids with accompanying tweets.\n",
    "        \"\"\"\n",
    "        tweets_by_id_lst = []\n",
    "        tokens_by_id = []\n",
    "        \n",
    "        with open(file, 'r') as f:\n",
    "            read = f.read()\n",
    "        \n",
    "            for tab_split in [line.split('\\t') for line in read.split('\\n')[:1000]]:\n",
    "                tweets_by_id_lst.append([tab_split[1], tab_split[-1]])\n",
    "        \n",
    "                for token in tab_split[-1].split():\n",
    "                    try:\n",
    "                        tokens_by_id.append([self.clean(token), tab_split[1]])\n",
    "                    except:\n",
    "                        break\n",
    "        \n",
    "        tokens_by_id = sorted(tokens_by_id)\n",
    "        tweets_by_id = {}\n",
    "        \n",
    "        for [ID, tweet] in sorted(tweets_by_id_lst):\n",
    "            tweets_by_id[ID] = tweet\n",
    "        \n",
    "        index = {}\n",
    "        postings_list = []\n",
    "        \n",
    "        for token,ID in tokens_by_id:\n",
    "            \n",
    "            if token not in index.keys():\n",
    "                #postings_list.append([ID])\n",
    "                index[token] = [1, LinkedList(ID)]\n",
    "                \n",
    "            elif ID not in index[token][1].to_list():\n",
    "                index[token][0]+=1\n",
    "                index[token][1].at_end(ID)\n",
    "            \n",
    "#             if token not in index.keys():\n",
    "#                 postings_list.append([ID])\n",
    "#                 index[token] = [1, postings_list[-1]]\n",
    "                \n",
    "#             elif ID not in index[token][1]:\n",
    "#                 index[token][0]+=1\n",
    "#                 index[token][1].append(ID)\n",
    "        \n",
    "        return index, postings_list, tweets_by_id\n",
    "    \n",
    "    def get_frequency(self, term):\n",
    "        \"\"\"\n",
    "        Get number of occurences of a certain term.\n",
    "        :param term: term for frequency query.\n",
    "        :return int: frequency as int.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return index.index[term][0]\n",
    "        except:\n",
    "            print('Term not found.')\n",
    "\n",
    "    def get_all_frequencies(self):\n",
    "        '''\n",
    "        Get frequencies of all terms in index.\n",
    "        :return frequencies: list of tuples containing frequency, term, sorted in descending order.\n",
    "        '''\n",
    "        frequencies = []\n",
    "        \n",
    "        for term in self.index.keys():\n",
    "            frequencies.append((self.get_frequency(term), term))\n",
    "            \n",
    "        return sorted(frequencies)[::-1]\n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Index('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
