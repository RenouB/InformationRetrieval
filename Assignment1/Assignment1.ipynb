{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class PostingsWrapper():\n",
    "    \"\"\"\n",
    "    This postings wrapper creates a link between the index dictionary and the postings list.\n",
    "    \"\"\"\n",
    "    def __init__(self, postings_list, posting, postings_index):\n",
    "        self.frequency = 1\n",
    "        self.postings_index = postings_index\n",
    "        postings_list.append([posting])\n",
    "        \n",
    "\n",
    "    def add_posting(self, postings_list, posting):\n",
    "        \"\"\"\n",
    "        \n",
    "        Adds a posting to the postings list, at correct index according to the term\n",
    "        Only called if the term has yet not corresponding postings.\n",
    "        \n",
    "        :param postings_list: postings list, an attribute of the index.\n",
    "        :param posting: the posting to be added, extracted from a list of tokens and docids.\n",
    "        :return: returns nothing\n",
    "        \"\"\"\n",
    "        if posting not in postings_list[self.postings_index]:\n",
    "            postings_list[self.postings_index].append(posting)\n",
    "            self.frequency += 1\n",
    "\n",
    "\n",
    "class index:\n",
    "    \"\"\"\n",
    "    Processes the tweets.csv file or any file containing the same structure, creates\n",
    "    an inverted index. This is a dictionary terms as keys and an instance of the PostingsWrapper \n",
    "    class as value. Also creates a seperate postings list, also as an attribute, which contains\n",
    "    all tweet ids where each term occured.\n",
    "    \"\"\"\n",
    "    def __init__(self, file):\n",
    "        \"\"\"\n",
    "        :param file: path to tweets.csv file.\n",
    "        \"\"\"\n",
    "        self.data = self.preprocess(file)\n",
    "        self.index, self.postings_list = self.create_index()\n",
    "\n",
    "    def preprocess(self, file):\n",
    "        \"\"\"\n",
    "        Opens raw text, spits it into lines comprised of six columns, stores in intermediary\n",
    "        tab_seperated variable.\n",
    "        Then proceeds to normalize this while transfering it to data variable. Everything is lowered\n",
    "        and compared to a regex which desires to only extract usernames and tokens containing \n",
    "        only letters. All irrelevant columns are disgarded.\n",
    "        \n",
    "        :param file: path to tweets.csv file.\n",
    "        :return: data, containing tweet IDs with corresponding tweets.\n",
    "        \"\"\"\n",
    "        raw_text = open(file).read()\n",
    "        tab_seperated = [item.split('\\t') for item in raw_text.split('\\n')]\n",
    "\n",
    "        for line in tab_seperated:\n",
    "            if len(line) == 1:\n",
    "                tab_seperated.remove(line)\n",
    "\n",
    "        data = []\n",
    "        for i in range(len(tab_seperated)):\n",
    "            data.append([tab_seperated[i][1], tab_seperated[i][4].lower()])\n",
    "\n",
    "        data = data[:5000]\n",
    "\n",
    "        for line in data:\n",
    "            line[1] = re.sub('https?:\\/\\/[^\\s]*|[^a-z\\s]', '', line[1])\n",
    "\n",
    "        return data\n",
    "\n",
    "    def create_index(self):\n",
    "        \"\"\"\n",
    "        Creates the index and postings list.\n",
    "        :return: index, a dictionary having a unique term as key and a PostingsWrapper instance\n",
    "        as value, and postings_list, a large list of lists containing all postings for each unique\n",
    "        term.\n",
    "        \"\"\"\n",
    "\n",
    "        # We initialize the index, the postings list, and an intermediary tokens_and_ids variable.\n",
    "        index = {}\n",
    "        postings_list = []\n",
    "        tokens_and_ids = []\n",
    "\n",
    "        # For each line in data, we split each tweet by whitespace into tokens.\n",
    "        # As a simple preprocessing step we check to make sure that the length of each token is\n",
    "        # > 0 before appending the token and its tweet ID to the tokens_and_ids list.\n",
    "        \n",
    "        for line in self.data[:100]:\n",
    "            for token in line[1].split():\n",
    "                if len(token) > 0:\n",
    "                    tokens_and_ids.append([token, line[0]])\n",
    "\n",
    "        # We sort our list of all tokens.\n",
    "        \n",
    "        tokens_and_ids.sort()\n",
    "\n",
    "        # The postings_index variable we initialize here will be used as we instantiate\n",
    "        # PostingsWrapper objects. This integer will enable us to keep track of the index\n",
    "        # of the postings list where all of a given term's postings are contained.\n",
    "        \n",
    "        postings_index = 0\n",
    "        \n",
    "        # For each line in tokens_and_ids, we check to make sure it is not already in our index.\n",
    "        # If it is not we add it, create a corresponding PostingsWrapper Object that will\n",
    "        # add to the postings list as it is initialized. The PostingsWrapper will also keep track\n",
    "        # of frequency for us.\n",
    "        # Having done this we then increment the postings_index variable by 1.\n",
    "        # If it is found that the term is already present in our index, we simply add the new \n",
    "        # posting to its postings list using the PostingsWrapper.add_posting method.\n",
    "        for line in tokens_and_ids:\n",
    "            if line[0] not in index.keys():\n",
    "                index[line[0]] = PostingsWrapper(postings_list, line[1], postings_index)\n",
    "                postings_index += 1\n",
    "            else:\n",
    "                index[line[0]].add_posting(postings_list, line[1])\n",
    "\n",
    "        return index, postings_list\n",
    "\n",
    "    def query_one(self, term):\n",
    "        \"\"\"\n",
    "        Queries for a term.\n",
    "        :param term: query term\n",
    "        :return: postings list corresponding to query term, or error message if no results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return [posting for posting in self.postings_list[index.index[term].postings_index]]\n",
    "        except:\n",
    "            print('No results for query.')\n",
    "        \n",
    "            \n",
    "    def query_and(self, term1, term2):\n",
    "        \"\"\"\n",
    "        Queries for the intersection of two terms.\n",
    "        :param term1: first term\n",
    "        :param term2: second term\n",
    "        :return: returns intersection of postings lists of both terms.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Here we access the postings list for each term, assign them to variables.\n",
    "        \n",
    "        postings1 = self.postings_list[index.index[term1].postings_index]\n",
    "        postings2 = self.postings_list[index.index[term1].postings_index]\n",
    "        \n",
    "        # Here we create iterators to help us compare the two postings lists.\n",
    "        \n",
    "        iterpostings1 = iter(postings1)\n",
    "        iterpostings2 = iter(postings2)\n",
    "        \n",
    "        # Here we initialize an empty intersection variable which will (hopefully) be filled.\n",
    "        intersection = []\n",
    "        \n",
    "        \n",
    "        current1 = next(iterpostings1)\n",
    "        current2 = next(iterpostings2)\n",
    "        \n",
    "        # This is the loop that iterates over the members of each postings list, comparing them.\n",
    "        # If there is a match it will be added to the intersection.\n",
    "        while True:\n",
    "\n",
    "            if current1 == current2:\n",
    "                intersection.append(current1)\n",
    "                try:\n",
    "                    next(iterpostings1)\n",
    "                    next(iterpostings2)\n",
    "                except:\n",
    "                    break\n",
    "            elif current1 < current2:\n",
    "                try:\n",
    "                    next(iterpostings1)\n",
    "                except:\n",
    "                    break\n",
    "            else:\n",
    "                try:\n",
    "                    next(iterpostings2)\n",
    "                except:\n",
    "                    break\n",
    "                    \n",
    "        return intersection\n",
    "    \n",
    "index = index('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['965672579133566980'] \n",
      "\n",
      "['965672579133566980'] \n",
      "\n",
      "['965672579133566980'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(index.query_one('pcr'),'\\n')\n",
    "print(index.query_one('centers'),'\\n')\n",
    "print(index.query_and('pcr', 'centers'),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do I want to remove?\n",
    "# numbers\n",
    "# punctuation\n",
    "# web adresses\n",
    "# emoticons\n",
    "# @ signs\n",
    "# all of this should be outside of names!!\n",
    "\n",
    "\n",
    "\n",
    "#regex = '(?<!([^\\s\\.]))[0-9]*|http:\\/\\/[^\\s]*|[!\"#\\$%&\\(\\)\\*\\+,-\\.\\/:;<=>\\?\\[\\\\\\]\\^`{\\|}~]+'\n",
    "# regex = '((?<!([^\\s\\.]))[0-9]*)*(http:\\/\\/[^\\s]*)*([!\"#\\$%&\\(\\)\\*\\+,-\\.\\/:;<=>\\?\\[\\\\\\]\\^`{\\|}~]+)*'\n",
    "# names = '(?<!([^\\s\\.]))[0-9]*'\n",
    "# websites = 'https?:\\/\\/[^\\s]*'\n",
    "# punctuation = '[!\"#$%&\\'()*+,-.\\/:;<=>?@\\[\\\\\\]^_`{|}~'\n",
    "# newline = '\\[newline\\]'\n",
    "# emoji = '[\\U00010000-\\U0010ffff]'\n",
    "# everything = '[\\U00010000-\\U0010ffff]|\\[newline\\]|[!\"#$%&\\'()*+,-.\\/:;<=>?@\\[\\\\\\]^_`{|}~|https?:\\/\\/[^\\s]*|(?<!([^\\s\\.]))[0-9]*'\n",
    "# everything_not = '[^@[^\\s]*[a-z]*]'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
